{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f1ec90e7-ccc9-439e-b712-33f3b29dfa7c",
   "metadata": {},
   "source": [
    "# GEOG 464 - Term Project - Cameron Brubacher"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "257fa0ba",
   "metadata": {},
   "source": [
    "## An interactive map of Canadian literature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87e79d34",
   "metadata": {},
   "source": [
    "### Importing modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f333e61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bs4 as bs\n",
    "import re\n",
    "import wikipedia as wiki\n",
    "import geocoder as geo\n",
    "import geopandas as gpd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2da55b16",
   "metadata": {},
   "source": [
    "### Importing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e11622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(articles_file, text_file):\n",
    "    # Import csv with articles from library database:\n",
    "    articles = pd.read_csv(articles_file)\n",
    "\n",
    "    # Import text version of html file with full-text citations from library database:\n",
    "    with open(text_file, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        record = [line.strip() for line in lines]\n",
    "    return articles, record"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c38580f",
   "metadata": {},
   "source": [
    "### Cleaning article data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eaa1eac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_art(dataset):\n",
    "    # Remove unnecessary/empty columns:\n",
    "    dataset.drop(columns={'ISSN', 'Publication Date', 'Issue', 'DOI', 'Doctype', 'Keywords', 'Abstract'}, inplace=True)\n",
    "    \n",
    "    # Rename ambiguous columns:\n",
    "    dataset.rename(columns={'Author': 'Journal Author', 'Publisher': 'Journal Publisher', 'PLink': 'Permalink'}, inplace=True)\n",
    "    \n",
    "    # Reformat article titles so all begin with 'Part n:':\n",
    "    #for i, row in dataset.iterrows():\n",
    "    #    if row[0][4] != ' ':\n",
    "    #        dataset.loc[i, 'Article Title'] = row[0][:4]+' '+row[0][4:]   \n",
    "    for i, row in dataset.iterrows():\n",
    "        if row[0][6] != ':':\n",
    "            dataset.loc[i, 'Article Title'] = row[0][:6]+':'+row[0][6:]\n",
    "    \n",
    "    # Reformat ISBN:\n",
    "    for i, row in dataset.iterrows():\n",
    "        dataset.loc[i, 'ISBN'] = row[3][2:-1]\n",
    "    \n",
    "    # Create empty work type and work sub-type columns:\n",
    "    dataset['Work Type'] = pd.Series(dtype='string')\n",
    "    dataset['Work Sub-Type'] = pd.Series(dtype='string')\n",
    "    \n",
    "    # Assign work type and work sub-types based on content of article titles:\n",
    "    for i, row in dataset.iterrows():\n",
    "        if row[0].count(';') >= 2:\n",
    "            dataset.loc[i, 'Work Type'] = row[0][((row[0].find(';'))+2):(row[0].rfind(';'))]\n",
    "            dataset.loc[i, 'Work Sub-Type'] = row[0][((row[0].rfind(';'))+2):]\n",
    "        elif (row[0].count(';') == 1) and (row[0].count(':') > 1):\n",
    "            dataset.loc[i, 'Work Type'] = row[0][((row[0].find(';'))+2):row[0].rfind(':')]\n",
    "            dataset.loc[i, 'Work Sub-Type'] = row[0][((row[0].rfind(':'))+2):]\n",
    "        elif row[0].count(';') == 1:\n",
    "            dataset.loc[i, 'Work Type'] = row[0][((row[0].rfind(';'))+2):]\n",
    "        elif row[0].count(':') > 2:\n",
    "            dataset.loc[i, 'Work Type'] = row[0][((row[0].find(':'))+2):(row[0].rfind(':'))]\n",
    "            dataset.loc[i, 'Work Sub-Type'] = row[0][((row[0].rfind(':'))+2):]\n",
    "        elif (row[0].count(';') == 0) and (row[0].count(':') == 1):\n",
    "            dataset.loc[i, 'Work Type'] = row[0][((row[0].rfind(':'))+2):]\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f2fb306",
   "metadata": {},
   "source": [
    "### Cleaning record data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab20ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_record(dataset):\n",
    "    # Remove entries based on length:\n",
    "    remove = {'Entry', 'publication', 'cross references', 'Copyright', 'Accessibility', 'Back' 'saved', 'Internet Explorer'}\n",
    "    delete = []\n",
    "    for i, entry in enumerate(dataset):\n",
    "        if len(entry) <= 3:\n",
    "            delete.append(i)\n",
    "    for n in sorted(delete, reverse=True):\n",
    "        del dataset[n]\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44f2ddb4",
   "metadata": {},
   "source": [
    "### Tabulating metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9e85a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_meta(dataset):\n",
    "    # Create list of headers based on sub-record metadata:\n",
    "    headers = []\n",
    "    for entry in dataset:\n",
    "        if entry.endswith(':'):\n",
    "            headers.append(entry)\n",
    "    \n",
    "    # Create empty metadata dataframe with list of headers:\n",
    "    meta = pd.DataFrame(data=[[None]*len(headers)], columns=headers)\n",
    "\n",
    "    # Add metadata to dataframe:\n",
    "    for i, entry in enumerate(dataset):\n",
    "        for header in headers:\n",
    "            try:\n",
    "                if (header == entry) and (dataset[i+1].endswith(':') == False):\n",
    "                    meta[header] = dataset[i+1]\n",
    "            except IndexError:\n",
    "                break\n",
    "    return meta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "609bf6f9",
   "metadata": {},
   "source": [
    "### Cleaning metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b51da6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_meta(dataset):\n",
    "    # Remove unnecessary/empty columns:\n",
    "    remove = {'Other Title:', 'Links:'}\n",
    "    for column in dataset.columns:\n",
    "        for string in remove:\n",
    "            if string == column:\n",
    "                dataset.drop(columns=column, inplace=True)\n",
    "\n",
    "    # Rename columns:\n",
    "    dataset.rename(columns={'Title:': 'Part Title', 'Record Type:': 'Record Type', 'Series Title:': 'Series Title', 'Author(s):': 'Series Author', 'Genre(s):': 'Series Genre', 'Subject(s):': 'Work Author', 'Book Source:': 'Source', 'Accession Number:': 'Accession Number', 'Database:': 'Database'}, inplace=True)\n",
    "    \n",
    "    # Reformat work author and series title columns:\n",
    "    for i, row in dataset.iterrows():\n",
    "        try:\n",
    "            author = dataset.loc[i, 'Work Author'][((dataset.loc[i, 'Work Author'].find(':'))+2):(dataset.loc[i, 'Work Author'].find(';'))]\n",
    "            author = author[((author.find(','))+2):]+author[:(author.find(','))]\n",
    "            dataset.loc[i, 'Work Author'] = author.title()\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d216ef5e",
   "metadata": {},
   "source": [
    "### Adding geographic and biographic information to metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geo_bio(dataset):\n",
    "    # Create column for author's birthplace:\n",
    "    dataset['Author Biography'] = pd.Series(dtype='string')\n",
    "    dataset['Author Birthplace'] = pd.Series(dtype='string')\n",
    "    dataset['latitude'] = pd.Series(dtype='float')\n",
    "    dataset['longitude'] = pd.Series(dtype='float')\n",
    "\n",
    "    for i, row in dataset.iterrows():\n",
    "        # Extract author's name:\n",
    "        author = dataset.loc[i, 'Work Author']\n",
    "        #print(author)\n",
    "\n",
    "        # Add short biography to dataframe:\n",
    "        try:\n",
    "            dataset.loc[i, 'Author Biography'] = wiki.summary(author, sentences=1, auto_suggest=True, redirect=True)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Add birthplace to dataframe:\n",
    "        if author == 'Alice Munro':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Wingham, Ontario'\n",
    "        if author == 'Anne Hebert':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Sainte-Catherine-de-la-Jacques-Cartier, Quebec'\n",
    "        if author == 'Earle Birney':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Calgary, Alberta'\n",
    "        if author == 'Ernest Buckler':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'West Dalhousie, Nova Scotia'\n",
    "        if author == 'Ethel Wilson':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Vancouver, British Columbia'\n",
    "        if author == 'Gabrielle Roy':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Saint Boniface, Manitoba'\n",
    "        if (author == 'Hugh Hood') or (author == 'Marian Engel') or (author == 'Morley Callaghan'):\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Toronto, Ontario'\n",
    "        if author == 'Margaret Atwood':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Ottawa, Ontario'\n",
    "        if author == 'Margaret Laurence':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Neepawa, Manitoba'\n",
    "        if (author == 'Mavis Gallant') or (author == 'Michael Ondaatje') or (author == 'Mordecai Richler'):\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Montreal, Quebec'\n",
    "        if author == 'Patricia K. Page':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Red Deer, Alberta'\n",
    "        if author == 'Robert Kroetsch':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Heisler, Alberta'\n",
    "        if author == 'Robertson Davies':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Thamesville, Ontario'\n",
    "        if author == 'Sinclair Ross':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Shellbrook, Saskatchewan'\n",
    "        if author == 'Thomas Raddall':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Halifax, Nova Scotia'\n",
    "        if author == 'W.O. Mitchell':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Weyburn, Saskatchewan'\n",
    "        if author == 'Leonard Cohen':\n",
    "            dataset.loc[i, 'Author Birthplace'] = 'Westmount, Quebec'\n",
    "        \n",
    "        # Extract author's birthplace:\n",
    "        place = dataset.loc[i, 'Author Birthplace']\n",
    "\n",
    "        # Geocode coordinates of birthplace:\n",
    "        location = geo.osm(place)\n",
    "        latlng = location.latlng\n",
    "\n",
    "        # Add coordinates to dataframe:\n",
    "        dataset.loc[i, 'latitude'] = latlng[0]\n",
    "        dataset.loc[i, 'longitude'] = latlng[1]\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab3c8e3c",
   "metadata": {},
   "source": [
    "### Tabulating citation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dcf41a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_cit(dataset):\n",
    "    # Create list of bibliographic entries:\n",
    "    works = []\n",
    "    for entry in dataset:\n",
    "        if re.search('^A\\d+', entry):\n",
    "            works.append(entry)\n",
    "    for i, entry in enumerate(works):\n",
    "        works[i] = entry.lstrip('A0123456789 ')\n",
    "    for i, entry in enumerate(works):\n",
    "        #print(entry)\n",
    "        if re.search('^[a-z]', entry):\n",
    "            works[i] = 'A'+entry\n",
    "    \n",
    "    # Split entries by and append to works list:\n",
    "    cits = []\n",
    "    for i, entry in enumerate(works):\n",
    "        row = entry.rsplit('.')\n",
    "        row = row[:-1]\n",
    "        for i, string in enumerate(row):\n",
    "            row[i] = string.strip()\n",
    "        cits.append(row)\n",
    "    return cits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "843723c6",
   "metadata": {},
   "source": [
    "### Cleaning citation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a235e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cit(dataset):\n",
    "    headers = ['Work Title', 'Year', 'Pages', 'Work Publisher', 'Publisher City', 'Notes']\n",
    "    cit = pd.DataFrame(index = range(len(dataset)), columns = headers)\n",
    "    for i, entry in enumerate(dataset):   \n",
    "        notes = \"\"\n",
    "        for i2, string in enumerate(entry):\n",
    "            if i2 == 0:\n",
    "                cit.loc[i, 'Work Title'] = string\n",
    "            elif (':' in string) and (',' in string):\n",
    "                cit.loc[i, 'Publisher City'] = string[:(string.find(':'))] \n",
    "                cit.loc[i, 'Work Publisher'] = string[((string.find(':'))+2):(string.rfind(','))] \n",
    "                cit.loc[i, 'Year'] = string[((string.rfind(','))+2):]     \n",
    "            elif 'pp' in string:\n",
    "                cit.loc[i, 'Pages'] = string\n",
    "            else:\n",
    "                notes = notes+'. '+string\n",
    "            cit.loc[i, 'Notes'] = notes\n",
    "    for i, row in cit.iterrows():\n",
    "        try:\n",
    "            if len(cit.loc[i, 'Notes']) < 1:\n",
    "                cit.loc[i, 'Notes'] = None\n",
    "            else:\n",
    "                cit.loc[i, 'Notes'] = cit.loc[i, 'Notes'][2:]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return cit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb50598c",
   "metadata": {},
   "source": [
    "### Calling functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d4261a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import citation and full-text html data:\n",
    "articles, record = import_data('data/articles.csv', 'data/EBSCOhost.txt')\n",
    "\n",
    "# Clean article and HTML record data:\n",
    "articles = clean_art(articles)\n",
    "record = clean_record(record)\n",
    "\n",
    "# Create list of split points (at 'Record: n') in HTML record data:\n",
    "splits = []\n",
    "for i, entry in enumerate(record):\n",
    "    if 'Record:' in entry:\n",
    "        splits.append(i)\n",
    "\n",
    "# Create empty metadata, citation, and joined dataframes:\n",
    "metadata = pd.DataFrame()\n",
    "citations = pd.DataFrame()\n",
    "metacit = pd.DataFrame()\n",
    "\n",
    "# Iterate through the HTML record data:\n",
    "for i, n in enumerate(splits):\n",
    "    # Create sub-record for each group of entries in HTML record data:\n",
    "    sub_rec = []\n",
    "    try:\n",
    "        sub_rec = record[n:(splits[i+1])]\n",
    "    except IndexError:\n",
    "        sub_rec = record[n:]\n",
    "        \n",
    "    # Tabulate and clean sub-record metadata:\n",
    "    meta = table_meta(sub_rec)\n",
    "    meta = clean_meta(meta)\n",
    "    meta = geo_bio(meta)\n",
    "\n",
    "    # Append sub-record metadata to metadata dataframe:\n",
    "    metadata = pd.concat([metadata, meta], ignore_index=True)\n",
    "\n",
    "    # Tabulate and clean sub-record citation data:\n",
    "    citlist = table_cit(sub_rec)\n",
    "    cit = clean_cit(citlist)\n",
    "\n",
    "    # Append sub-record citation data to citation dataframe:\n",
    "    citations = pd.concat([citations, cit], ignore_index=True)\n",
    "\n",
    "    # Append sub-record citation data to sub-record metadata:\n",
    "    metas = pd.concat([meta]*(len(cit)), ignore_index=True)\n",
    "    submetacit = metas.join(cit)\n",
    "\n",
    "    # Append sub-record metadata and citation data to joined dataframe:\n",
    "    metacit = pd.concat([metacit, submetacit], ignore_index=True)\n",
    "\n",
    "# Merge articles dataframe to join metadata-citation dataframe to creation master dataframe:\n",
    "master = articles.merge(metacit, on='Accession Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "047aa7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert master dataframe to geodataframe:\n",
    "geometry = gpd.points_from_xy(master.longitude, master.latitude)\n",
    "master_gdf = gpd.GeoDataFrame(master, geometry=geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6968f5cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
